{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> MobileNetV2 x1.4 Quantization </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we quantize our pre-trained , 3 times adversarial trained MobileNetV2 model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries and Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "import time\n",
    "from math import log10, sqrt\n",
    "from torch.utils.data import DataLoader\n",
    "from art.utils import load_cifar10\n",
    "import random\n",
    "import urllib\n",
    "from torch.quantization import MovingAverageMinMaxObserver\n",
    "from torch.ao.quantization.observer import MinMaxObserver\n",
    "from torch.quantization import QuantStub, DeQuantStub\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.transforms.functional as F\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "\n",
    "def test(model: nn.Module, dataloader: DataLoader, cuda=False) -> float:\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in dataloader:\n",
    "            inputs, labels = data\n",
    "            if cuda:\n",
    "              inputs = inputs.cuda()\n",
    "              labels = labels.cuda()\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return 100 * correct / total\n",
    "\n",
    "def normalize_np(img):\n",
    "  img = torch.from_numpy(img)\n",
    "  img = F.normalize(img, [0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "  return img.numpy()\n",
    "\n",
    "def custom_collate(batch):\n",
    "        # Combine a list of samples into a batch\n",
    "        data, labels = zip(*batch)\n",
    "        data = torch.stack(data)\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        return data, labels\n",
    "\n",
    "def evaluator(model, loader):\n",
    "  model.eval()\n",
    "  top_1 = 0\n",
    "  top_5 = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      inputs, labels = data\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      _, predicted = torch.max(outputs, 1, keepdim=True)\n",
    "      top_1 += torch.sum(predicted.view(-1) == labels).item()\n",
    "\n",
    "      _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
    "      top_5 += torch.sum(predicted_5 == labels.unsqueeze(1)).item()\n",
    "\n",
    "  return (\"{:.2f}\".format((top_1/400) * 100), \"{:.2f}\".format((top_5/400) * 100))\n",
    "\n",
    "def evaluator_testset(model, loader):\n",
    "  model.eval()\n",
    "  top_1 = 0\n",
    "  top_5 = 0\n",
    "  with torch.no_grad():\n",
    "    for data in loader:\n",
    "      inputs, labels = data\n",
    "      outputs = model(inputs)\n",
    "\n",
    "      _, predicted = torch.max(outputs, 1, keepdim=True)\n",
    "      top_1 += torch.sum(predicted.view(-1) == labels).item()\n",
    "\n",
    "      _, predicted_5 = torch.topk(outputs, k=5, dim=1)\n",
    "      top_5 += torch.sum(predicted_5 == labels.unsqueeze(1)).item()\n",
    "\n",
    "  return (\"{:.2f}\".format((top_1/10000) * 100), \"{:.2f}\".format((top_5/10000) * 100))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test), min_pixel_value, max_pixel_value = load_cifar10()\n",
    "\n",
    "x_train = np.transpose(x_train, (0, 3, 1, 2)).astype(np.float32)\n",
    "x_test = np.transpose(x_test, (0, 3, 1, 2)).astype(np.float32)\n",
    "\n",
    "classes_cifar = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "y_test_set = np.zeros((400,),np.int8)\n",
    "\n",
    "y_train_set = np.zeros((400,),np.int8)\n",
    "\n",
    "\n",
    "for i in range(400):\n",
    "        y_test_set[i] = np.where(y_test[i] == 1)[0][0]\n",
    "\n",
    "for i in range(400):\n",
    "        y_train_set[i] = np.where(y_train[i] == 1)[0][0]\n",
    "\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=64,shuffle=False, num_workers=2, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition\n",
    "\n",
    "Here we change += operations with the float functional equivalent for VitisAI support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "from typing import Dict, Type, Any, Callable, Union, List, Optional\n",
    "from torch import Tensor\n",
    "try:\n",
    "    from torch.hub import load_state_dict_from_url\n",
    "except ImportError:\n",
    "    from torch.utils.model_zoo import load_url as load_state_dict_from_url\n",
    "from torch.ao.nn.quantized.modules.functional_modules import FloatFunctional\n",
    "\n",
    "def _make_divisible(v: float, divisor: int, min_value: Optional[int] = None) -> int:\n",
    "    \"\"\"\n",
    "    This function is taken from the original tf repo.\n",
    "    It ensures that all layers have a channel number that is divisible by 8\n",
    "    It can be seen here:\n",
    "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
    "    \"\"\"\n",
    "    if min_value is None:\n",
    "        min_value = divisor\n",
    "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_v < 0.9 * v:\n",
    "        f_add = torch.nn.quantized.FloatFunctional()\n",
    "        new_v = f_add.add(new_v, divisor)\n",
    "        #new_v += divisor\n",
    "    return new_v\n",
    "\n",
    "\n",
    "class ConvBNActivation(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_planes: int,\n",
    "        out_planes: int,\n",
    "        kernel_size: int = 3,\n",
    "        stride: int = 1,\n",
    "        groups: int = 1,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        activation_layer: Optional[Callable[..., nn.Module]] = None,\n",
    "        dilation: int = 1,\n",
    "    ) -> None:\n",
    "        padding = (kernel_size - 1) // 2 * dilation\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if activation_layer is None:\n",
    "            activation_layer = nn.ReLU6\n",
    "        super(ConvBNReLU, self).__init__(\n",
    "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, dilation=dilation, groups=groups,\n",
    "                      bias=False),\n",
    "            norm_layer(out_planes),\n",
    "            activation_layer(inplace=True)\n",
    "        )\n",
    "        self.out_channels = out_planes\n",
    "\n",
    "\n",
    "# necessary for backwards compatibility\n",
    "ConvBNReLU = ConvBNActivation\n",
    "\n",
    "class InvertedResidual(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        inp: int,\n",
    "        oup: int,\n",
    "        stride: int,\n",
    "        expand_ratio: int,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        super(InvertedResidual, self).__init__()\n",
    "        self.stride = stride\n",
    "        assert stride in [1, 2]\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        hidden_dim = int(round(inp * expand_ratio))\n",
    "        self.use_res_connect = self.stride == 1 and inp == oup\n",
    "\n",
    "        layers: List[nn.Module] = []\n",
    "        if expand_ratio != 1:\n",
    "            # pw\n",
    "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1, norm_layer=norm_layer))\n",
    "        layers.extend([\n",
    "            # dw\n",
    "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim, norm_layer=norm_layer),\n",
    "            # pw-linear\n",
    "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
    "            norm_layer(oup),\n",
    "        ])\n",
    "        self.conv = nn.Sequential(*layers)\n",
    "        self.out_channels = oup\n",
    "        self._is_cn = stride > 1\n",
    "        self.ff = torch.nn.quantized.FloatFunctional()\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        if self.use_res_connect:\n",
    "            #return x + self.conv(x)\n",
    "            return self.ff.add(x, self.conv(x))\n",
    "        else:\n",
    "            return self.conv(x)\n",
    "\n",
    "\n",
    "class MobileNetV2(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_classes: int = 10,\n",
    "        width_mult: float = 1.0,\n",
    "        inverted_residual_setting: Optional[List[List[int]]] = None,\n",
    "        round_nearest: int = 8,\n",
    "        block: Optional[Callable[..., nn.Module]] = None,\n",
    "        norm_layer: Optional[Callable[..., nn.Module]] = None\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_classes (int): Number of classes\n",
    "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
    "            inverted_residual_setting: Network structure\n",
    "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
    "            Set to 1 to turn off rounding\n",
    "            block: Module specifying inverted residual building block for mobilenet\n",
    "            norm_layer: Module specifying the normalization layer to use\n",
    "\n",
    "        \"\"\"\n",
    "        super(MobileNetV2, self).__init__()\n",
    "\n",
    "        if block is None:\n",
    "            block = InvertedResidual\n",
    "\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "\n",
    "        input_channel = 32\n",
    "        last_channel = 1280\n",
    "\n",
    "        if inverted_residual_setting is None:\n",
    "            inverted_residual_setting = [\n",
    "                # t, c, n, s\n",
    "                [1, 16, 1, 1],\n",
    "                [6, 24, 2, 1],  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
    "                [6, 32, 3, 2],\n",
    "                [6, 64, 4, 2],\n",
    "                [6, 96, 3, 1],\n",
    "                [6, 160, 3, 2],\n",
    "                [6, 320, 1, 1],\n",
    "            ]\n",
    "\n",
    "        # only check the first element, assuming user knows t,c,n,s are required\n",
    "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
    "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
    "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
    "\n",
    "        # building first layer\n",
    "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
    "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
    "        features: List[nn.Module] = [ConvBNReLU(3, input_channel, stride=1, norm_layer=norm_layer)]  # NOTE: change stride 2 -> 1 for CIFAR10/100\n",
    "        # building inverted residual blocks\n",
    "        for t, c, n, s in inverted_residual_setting:\n",
    "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
    "            for i in range(n):\n",
    "                stride = s if i == 0 else 1\n",
    "                features.append(block(input_channel, output_channel, stride, expand_ratio=t, norm_layer=norm_layer))\n",
    "                input_channel = output_channel\n",
    "        # building last several layers\n",
    "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1, norm_layer=norm_layer))\n",
    "        # make it nn.Sequential\n",
    "        self.features = nn.Sequential(*features)\n",
    "\n",
    "        # building classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(self.last_channel, num_classes),\n",
    "        )\n",
    "\n",
    "        # weight initialization\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.ones_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def _forward_impl(self, x: Tensor) -> Tensor:\n",
    "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
    "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
    "        x = self.features(x)\n",
    "        # Cannot use \"squeeze\" as batch-size can be 1\n",
    "        x = nn.functional.adaptive_avg_pool2d(x, (1, 1))\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x: Tensor) -> Tensor:\n",
    "        return self._forward_impl(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"REMINDER: Load the retrained model, not the initial.\"\"\"\n",
    "\n",
    "\n",
    "adversarial_state1 = urllib.request.urlopen(\"https://drive.usercontent.google.com/download?id=1umVJExm8VeVSqIpEWlCVhMbJrNZeU-aw&export=download&confirm=t&uuid=0\")\n",
    "\n",
    "\"\"\"\n",
    "If you want to manually load the file or load your own model version, use the code below updating your path accordingly\n",
    "\n",
    "# Adversarial Model 1 https://drive.google.com/file/d/1umVJExm8VeVSqIpEWlCVhMbJrNZeU-aw/view?usp=sharing\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#path = \"~/Downloads/\"\n",
    "\n",
    "#adversarial_state1 = os.path.join(path, \"MobileNet_1it_CIFAR10_94.17acc.pkl\")\n",
    "\n",
    "\n",
    "adversarial_state2 = urllib.request.urlopen(\"https://drive.usercontent.google.com/download?id=12Ux4pWLxK4gTr54eMN-Mds4adPMvjfNX&export=download&confirm=t&uuid=0\")\n",
    "\n",
    "\"\"\"\n",
    "If you want to manually load the file or load your own model version, use the code below updating your path accordingly\n",
    "\n",
    "# Adversarial Model 2 https://drive.google.com/file/d/12Ux4pWLxK4gTr54eMN-Mds4adPMvjfNX/view?usp=sharing\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#path = \"~/Downloads/\"\n",
    "\n",
    "#adversarial_state2 = os.path.join(path, \"MobileNet_2it_CIFAR10_93.58acc.pkl\")\n",
    "\n",
    "\n",
    "\n",
    "adversarial_state3 = urllib.request.urlopen(\"https://drive.usercontent.google.com/download?id=19Nxyh3nIwR0KdXv_ZBk8Ur-DtQzuuct3&export=download&confirm=t&uuid=0\")\n",
    "\n",
    "\"\"\"\n",
    "If you want to manually load the file or load your own model version, use the code below updating your path accordingly\n",
    "\n",
    "# Adversarial Model 3 https://drive.google.com/file/d/19Nxyh3nIwR0KdXv_ZBk8Ur-DtQzuuct3/view?usp=sharing\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#path = \"~/Downloads/\"\n",
    "\n",
    "#adversarial_state3 = os.path.join(path, \"MobileNet_3it_CIFAR10_92.79acc.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Adversarial Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_fp_model = MobileNetV2(width_mult = 1.4)\n",
    "\n",
    "mobilenet_fp_model.load_state_dict(torch.load(adversarial_state1,map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "mobilenet_model = MobileNetV2(width_mult = 1.4)\n",
    "\n",
    "mobilenet_model.load_state_dict(torch.load(adversarial_state1,map_location=torch.device('cpu')))\n",
    "\n",
    "mobilenet_model = torch.quantization.QuantWrapper(mobilenet_model)\n",
    "B=8\n",
    "mobilenet_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
    "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
    "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
    "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
    "torch.quantization.prepare(mobilenet_model, inplace=True)\n",
    "\n",
    "mobilenet_model.to(\"cpu\")\n",
    "test(mobilenet_model, testloader, cuda=False)\n",
    "mobilenet_model.to(\"cpu\")\n",
    "\n",
    "torch.quantization.convert(mobilenet_model, inplace=True)\n",
    "\n",
    "top1 , top5 = evaluator_testset(mobilenet_fp_model, testloader)\n",
    "print('Adveraral Training 1: Initial accuracy on test images: Top-1: {}%  Top-5: {}% - FP32'.format(top1, top5))\n",
    "\n",
    "top1, top5 = evaluator_testset(mobilenet_model, testloader)\n",
    "print('Adveraral Training 1 Quantized: accuracy on test images: Top-1: {}%  Top-5: {}% - INT8'.format(top1, top5))\n",
    "\n",
    "torch.save(mobilenet_model.state_dict(), \"MobileNetV2_adv1_quant_CIF10.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Adversarial Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_fp_model = MobileNetV2(width_mult = 1.4)\n",
    "\n",
    "mobilenet_fp_model.load_state_dict(torch.load(adversarial_state2,map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "mobilenet_model = MobileNetV2(width_mult = 1.4)\n",
    "\n",
    "mobilenet_model.load_state_dict(torch.load(adversarial_state2,map_location=torch.device('cpu')))\n",
    "\n",
    "mobilenet_model = torch.quantization.QuantWrapper(mobilenet_model)\n",
    "B=8\n",
    "mobilenet_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
    "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
    "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
    "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
    "torch.quantization.prepare(mobilenet_model, inplace=True)\n",
    "\n",
    "mobilenet_model.to(\"cpu\")\n",
    "test(mobilenet_model, testloader, cuda=False)\n",
    "mobilenet_model.to(\"cpu\")\n",
    "\n",
    "torch.quantization.convert(mobilenet_model, inplace=True)\n",
    "\n",
    "top1 , top5 = evaluator_testset(mobilenet_fp_model, testloader)\n",
    "print('Adveraral Training 2: Initial accuracy on test images: Top-1: {}%  Top-5: {}% - FP32'.format(top1, top5))\n",
    "\n",
    "top1, top5 = evaluator_testset(mobilenet_model, testloader)\n",
    "print('Adveraral Training 2 Quantized: accuracy on test images: Top-1: {}%  Top-5: {}% - INT8'.format(top1, top5))\n",
    "\n",
    "\n",
    "torch.save(mobilenet_model.state_dict(), \"MobileNetV2_adv2_quant_CIF10.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Third Adversarial Training Iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mobilenet_fp_model = MobileNetV2(width_mult = 1.4)\n",
    "\n",
    "mobilenet_fp_model.load_state_dict(torch.load(adversarial_state3,map_location=torch.device('cpu')))\n",
    "\n",
    "\n",
    "mobilenet_model = MobileNetV2(width_mult = 1.4)\n",
    "\n",
    "mobilenet_model.load_state_dict(torch.load(adversarial_state3,map_location=torch.device('cpu')))\n",
    "\n",
    "mobilenet_model = torch.quantization.QuantWrapper(mobilenet_model)\n",
    "B=8\n",
    "mobilenet_model.qconfig = torch.quantization.QConfig(activation= MovingAverageMinMaxObserver.with_args(quant_min=0, quant_max=int(2 ** B - 1), dtype=torch.quint8,\n",
    "                                                              qscheme=torch.per_tensor_affine, reduce_range=False),\n",
    "                                                     weight= MovingAverageMinMaxObserver.with_args(quant_min=int(-(2 ** B) / 2), quant_max=int((2 ** B) / 2 - 1), dtype=torch.qint8,\n",
    "                                                              qscheme=torch.per_tensor_symmetric, reduce_range=False))\n",
    "torch.quantization.prepare(mobilenet_model, inplace=True)\n",
    "\n",
    "mobilenet_model.to(\"cpu\")\n",
    "test(mobilenet_model, testloader, cuda=False)\n",
    "mobilenet_model.to(\"cpu\")\n",
    "\n",
    "torch.quantization.convert(mobilenet_model, inplace=True)\n",
    "\n",
    "top1 , top5 = evaluator_testset(mobilenet_fp_model, testloader)\n",
    "print('Adveraral Training 3: Initial accuracy on test images: Top-1: {}%  Top-5: {}% - FP32'.format(top1, top5))\n",
    "\n",
    "top1, top5 = evaluator_testset(mobilenet_model, testloader)\n",
    "print('Adveraral Training 3 Quantized: accuracy on test images: Top-1: {}%  Top-5: {}% - INT8'.format(top1, top5))\n",
    "\n",
    "torch.save(mobilenet_model.state_dict(), \"MobileNetV2_adv3_quant_CIF10.pkl\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
